{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"419b97e53a9842f7ae242e4249dc3797":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0ee608c218b649a7916fbee284760539","IPY_MODEL_2df73a2904544963a11c1fad9f813740","IPY_MODEL_b94c3bfc73564bb98dc424718cec3a58"],"layout":"IPY_MODEL_f1a37e256c084e0e80afba84994fd1a7"}},"0ee608c218b649a7916fbee284760539":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_358569e5d3b94e3fb4522a0a35b6084e","placeholder":"​","style":"IPY_MODEL_2cc58540b2754481b3ac83c137c2ba29","value":"Downloading (…)lve/main/config.json: 100%"}},"2df73a2904544963a11c1fad9f813740":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a67a910967024ee7a4eca678696b34c5","max":632,"min":0,"orientation":"horizontal","style":"IPY_MODEL_43542f8f319c4812b56318ed3e71dbed","value":632}},"b94c3bfc73564bb98dc424718cec3a58":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bd3c10661cfe4253a7df491f05d2335a","placeholder":"​","style":"IPY_MODEL_c7a0e6aa4f9b4f898ea2bfdb581e19b2","value":" 632/632 [00:00&lt;00:00, 34.4kB/s]"}},"f1a37e256c084e0e80afba84994fd1a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"358569e5d3b94e3fb4522a0a35b6084e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2cc58540b2754481b3ac83c137c2ba29":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a67a910967024ee7a4eca678696b34c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43542f8f319c4812b56318ed3e71dbed":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bd3c10661cfe4253a7df491f05d2335a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7a0e6aa4f9b4f898ea2bfdb581e19b2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"efaad66443e642b8a781d2a7bdad8220":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3e203b1662e64492a2057d4520fa0368","IPY_MODEL_ae85c3386d5d41b1afe0efc511ff94ef","IPY_MODEL_a1f4179ff58042e4bed123c98884ac0d"],"layout":"IPY_MODEL_950d893c4a4e4311a5b1a20aa05b2b60"}},"3e203b1662e64492a2057d4520fa0368":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_407368c5a9a746acad253726bd58809a","placeholder":"​","style":"IPY_MODEL_a31387adc7904ba28185a0fe8c1a78f1","value":"Downloading pytorch_model.bin: 100%"}},"ae85c3386d5d41b1afe0efc511ff94ef":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_08ea5244434a483696ec055f4df09b54","max":47679974,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dc60e7c734c1439caea37c4eaeb81f99","value":47679974}},"a1f4179ff58042e4bed123c98884ac0d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b209c059ebdc4f2495392515d3645f46","placeholder":"​","style":"IPY_MODEL_7e30d332ebaf4ef7a5c425a859680239","value":" 47.7M/47.7M [00:00&lt;00:00, 178MB/s]"}},"950d893c4a4e4311a5b1a20aa05b2b60":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"407368c5a9a746acad253726bd58809a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a31387adc7904ba28185a0fe8c1a78f1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"08ea5244434a483696ec055f4df09b54":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc60e7c734c1439caea37c4eaeb81f99":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b209c059ebdc4f2495392515d3645f46":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e30d332ebaf4ef7a5c425a859680239":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9044e3e2ba7742898681f8c2cab10e1c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e4e3148628544961a843b296135d5fb5","IPY_MODEL_3d522b73fae649dab46588e2e764aae4","IPY_MODEL_c5d4790e629f4709aa45c5158ef3adf4"],"layout":"IPY_MODEL_76d589e8d6634a7ea67920d26d5bec3e"}},"e4e3148628544961a843b296135d5fb5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b02cc6c2bb6040d1a1082311917cd293","placeholder":"​","style":"IPY_MODEL_9d47441331ef4c6a98cbe5e585de060a","value":"100%"}},"3d522b73fae649dab46588e2e764aae4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b7d09f100dc4e17b217a84bd767c2c1","max":13126,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0d6f28e70b2247eca1ec5cfa2d36bdd7","value":13126}},"c5d4790e629f4709aa45c5158ef3adf4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d65b8539e6de45d89b308858cf67ec22","placeholder":"​","style":"IPY_MODEL_967cc44586cc4c988237c34ce37cf8c6","value":" 13126/13126 [1:11:38&lt;00:00,  3.46it/s]"}},"76d589e8d6634a7ea67920d26d5bec3e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b02cc6c2bb6040d1a1082311917cd293":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d47441331ef4c6a98cbe5e585de060a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0b7d09f100dc4e17b217a84bd767c2c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d6f28e70b2247eca1ec5cfa2d36bdd7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d65b8539e6de45d89b308858cf67ec22":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"967cc44586cc4c988237c34ce37cf8c6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/dariush-bahrami/character-tokenizer.git","metadata":{"id":"5HmwpqzwnMyE","colab":{"base_uri":"https://localhost:8080/"},"outputId":"357e52b5-5f6d-4263-f774-4dcbdb0adad9","execution":{"iopub.status.busy":"2023-04-27T10:54:08.461062Z","iopub.execute_input":"2023-04-27T10:54:08.461906Z","iopub.status.idle":"2023-04-27T10:54:10.007391Z","shell.execute_reply.started":"2023-04-27T10:54:08.461872Z","shell.execute_reply":"2023-04-27T10:54:10.005882Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Cloning into 'character-tokenizer'...\nremote: Enumerating objects: 16, done.\u001b[K\nremote: Counting objects: 100% (16/16), done.\u001b[K\nremote: Compressing objects: 100% (10/10), done.\u001b[K\nremote: Total 16 (delta 3), reused 12 (delta 3), pack-reused 0\u001b[K\nReceiving objects: 100% (16/16), 5.13 KiB | 1.28 MiB/s, done.\nResolving deltas: 100% (3/3), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"# !pip install transformers","metadata":{"id":"hkVexA2nnRQ5","execution":{"iopub.status.busy":"2023-04-27T10:29:01.480965Z","iopub.execute_input":"2023-04-27T10:29:01.481282Z","iopub.status.idle":"2023-04-27T10:29:01.489250Z","shell.execute_reply.started":"2023-04-27T10:29:01.481247Z","shell.execute_reply":"2023-04-27T10:29:01.488028Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import numpy as np\ndevice = 'cuda'","metadata":{"id":"z3o8b2onhooe","execution":{"iopub.status.busy":"2023-04-27T10:54:10.009265Z","iopub.execute_input":"2023-04-27T10:54:10.009714Z","iopub.status.idle":"2023-04-27T10:54:10.016266Z","shell.execute_reply.started":"2023-04-27T10:54:10.009661Z","shell.execute_reply":"2023-04-27T10:54:10.014902Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import string\nimport sys\nsys.path.append(\"/kaggle/working/character-tokenizer\")\nfrom charactertokenizer import CharacterTokenizer\n\nchars = \"АаБбВвГгДдЕеЁёЖжЗзИиЙйКкЛлМмНнОоПпРрСсТтУуФфХхЦцЧчШшЩщЪъЫыЬьЭэЮюЯя\"\nmodel_max_length = 64\ntokenizer = CharacterTokenizer(chars, model_max_length)","metadata":{"id":"5FaCG9ajnS_G","execution":{"iopub.status.busy":"2023-04-27T10:54:10.019996Z","iopub.execute_input":"2023-04-27T10:54:10.020770Z","iopub.status.idle":"2023-04-27T10:54:11.671853Z","shell.execute_reply.started":"2023-04-27T10:54:10.020728Z","shell.execute_reply":"2023-04-27T10:54:11.670818Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"example = \"Привет\"\ntokens = tokenizer(example)\nprint(tokens)","metadata":{"id":"I5FSPMOSncpI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"298569ea-c34c-46f4-acdd-d31ff08e7943","execution":{"iopub.status.busy":"2023-04-27T10:54:11.673340Z","iopub.execute_input":"2023-04-27T10:54:11.674075Z","iopub.status.idle":"2023-04-27T10:54:11.682643Z","shell.execute_reply.started":"2023-04-27T10:54:11.674034Z","shell.execute_reply":"2023-04-27T10:54:11.681464Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"{'input_ids': [0, 39, 42, 26, 12, 18, 46, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer.convert_ids_to_tokens(tokens['input_ids'])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AioDA4WOMV66","outputId":"d6a8dff6-6afd-4278-e80c-2f3c091226a3","execution":{"iopub.status.busy":"2023-04-27T10:54:11.684086Z","iopub.execute_input":"2023-04-27T10:54:11.685156Z","iopub.status.idle":"2023-04-27T10:54:11.694409Z","shell.execute_reply.started":"2023-04-27T10:54:11.685120Z","shell.execute_reply":"2023-04-27T10:54:11.693299Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"['[CLS]', 'П', 'р', 'и', 'в', 'е', 'т', '[SEP]']"},"metadata":{}}]},{"cell_type":"markdown","source":"Задание: обучите модель классификации букв для задачи расстановки ударения с помощью методов из библиотеки transformers. Датасет для обучения можно взять отсюда: https://github.com/Koziev/NLP_Datasets/blob/master/Stress/all_accents.zip\n\n1. Напишите класс для Dataset/Dataloder и азбейте данные на случайные train / test сплиты в соотношении 50:50. (1 балл)\n2. Попробуйте несколько моделей: Bert, Albert, Deberta. (3 балла)\nПример конфигурации для deberta: https://huggingface.co/IlyaGusev/ru-word-stress-transformer/blob/main/config.json","metadata":{"id":"KQkp36rEoScR"}},{"cell_type":"code","source":"# !unzip all_accents.zip","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gdTKW8M3NEpD","outputId":"4e8dfcf7-92ab-4174-fe3f-4766e571d1f2","execution":{"iopub.status.busy":"2023-04-27T10:54:11.695865Z","iopub.execute_input":"2023-04-27T10:54:11.696746Z","iopub.status.idle":"2023-04-27T10:54:11.702713Z","shell.execute_reply.started":"2023-04-27T10:54:11.696704Z","shell.execute_reply":"2023-04-27T10:54:11.701644Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv('/kaggle/input/ska-sdelai-dataset-mraz/all_accents/all_accents.tsv', delimiter='\\t')\ndf.shape[0]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pPA_V4JqNEmx","outputId":"4aa859af-355c-44e8-e80e-9d35a19e7d12","execution":{"iopub.status.busy":"2023-04-27T10:54:11.704071Z","iopub.execute_input":"2023-04-27T10:54:11.704957Z","iopub.status.idle":"2023-04-27T10:54:14.903585Z","shell.execute_reply.started":"2023-04-27T10:54:11.704917Z","shell.execute_reply":"2023-04-27T10:54:14.902453Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"1680534"},"metadata":{}}]},{"cell_type":"code","source":"df.sample(10)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"id":"gz7ORlPxNEkd","outputId":"303ce30b-f563-44ee-9132-66f1dbd8d4cc","execution":{"iopub.status.busy":"2023-04-27T10:54:14.905017Z","iopub.execute_input":"2023-04-27T10:54:14.905689Z","iopub.status.idle":"2023-04-27T10:54:14.974944Z","shell.execute_reply.started":"2023-04-27T10:54:14.905648Z","shell.execute_reply":"2023-04-27T10:54:14.973725Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                      -де               -д^е\n818907       обмачивается      обм^ачивается\n753609          невесомое         невес^омое\n826637     обосновываемая    обосн^овываемая\n917176   отретушированное  отретуш^ированное\n1162063      приладожскую      прил^адожскую\n1020762          побешусь          побеш^усь\n1304621         расценкам         расц^енкам\n89748          бессмыслен        бессм^ыслен\n1457744    субарендаторов    субаренд^аторов\n904182           отксерим          откс^ерим","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>-де</th>\n      <th>-д^е</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>818907</th>\n      <td>обмачивается</td>\n      <td>обм^ачивается</td>\n    </tr>\n    <tr>\n      <th>753609</th>\n      <td>невесомое</td>\n      <td>невес^омое</td>\n    </tr>\n    <tr>\n      <th>826637</th>\n      <td>обосновываемая</td>\n      <td>обосн^овываемая</td>\n    </tr>\n    <tr>\n      <th>917176</th>\n      <td>отретушированное</td>\n      <td>отретуш^ированное</td>\n    </tr>\n    <tr>\n      <th>1162063</th>\n      <td>приладожскую</td>\n      <td>прил^адожскую</td>\n    </tr>\n    <tr>\n      <th>1020762</th>\n      <td>побешусь</td>\n      <td>побеш^усь</td>\n    </tr>\n    <tr>\n      <th>1304621</th>\n      <td>расценкам</td>\n      <td>расц^енкам</td>\n    </tr>\n    <tr>\n      <th>89748</th>\n      <td>бессмыслен</td>\n      <td>бессм^ыслен</td>\n    </tr>\n    <tr>\n      <th>1457744</th>\n      <td>субарендаторов</td>\n      <td>субаренд^аторов</td>\n    </tr>\n    <tr>\n      <th>904182</th>\n      <td>отксерим</td>\n      <td>откс^ерим</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df['target_idx'] = [word.find('^')for word in df['-д^е']]\ndf.sample(5)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"cY9CfGjdngVw","outputId":"99f590af-5d03-4901-b8cd-f06e0cc1b1fb","execution":{"iopub.status.busy":"2023-04-27T10:54:14.979041Z","iopub.execute_input":"2023-04-27T10:54:14.979341Z","iopub.status.idle":"2023-04-27T10:54:15.824630Z","shell.execute_reply.started":"2023-04-27T10:54:14.979314Z","shell.execute_reply":"2023-04-27T10:54:15.823482Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                 -де          -д^е  target_idx\n901705       отклеен      откл^еен           4\n1212848     пропивай     пропив^ай           6\n1595625  харьковчане  харьковч^ане           8\n1076979  поконченная  пок^онченная           3\n1474462    тактильно    такт^ильно           4","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>-де</th>\n      <th>-д^е</th>\n      <th>target_idx</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>901705</th>\n      <td>отклеен</td>\n      <td>откл^еен</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1212848</th>\n      <td>пропивай</td>\n      <td>пропив^ай</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1595625</th>\n      <td>харьковчане</td>\n      <td>харьковч^ане</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>1076979</th>\n      <td>поконченная</td>\n      <td>пок^онченная</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1474462</th>\n      <td>тактильно</td>\n      <td>такт^ильно</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.drop(index=np.where(df.target_idx == -1)[0], inplace=True)\ndf.shape[0]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g4qJFVFdnxaY","outputId":"c60bcca8-8195-4754-8f89-9a45dcbc76c1","execution":{"iopub.status.busy":"2023-04-27T10:54:15.826096Z","iopub.execute_input":"2023-04-27T10:54:15.826996Z","iopub.status.idle":"2023-04-27T10:54:15.969531Z","shell.execute_reply.started":"2023-04-27T10:54:15.826953Z","shell.execute_reply":"2023-04-27T10:54:15.968505Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"1680027"},"metadata":{}}]},{"cell_type":"code","source":"train_words = df['-де']\ntarget = df.target_idx","metadata":{"id":"MID6Scn_bFnQ","execution":{"iopub.status.busy":"2023-04-27T10:54:15.971053Z","iopub.execute_input":"2023-04-27T10:54:15.971485Z","iopub.status.idle":"2023-04-27T10:54:15.976810Z","shell.execute_reply.started":"2023-04-27T10:54:15.971445Z","shell.execute_reply":"2023-04-27T10:54:15.975711Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"CKOqLey3pmB2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_len = 0\nfor word in train_words:\n    input_ids = tokenizer(word)['input_ids']\n    max_len = max(max_len, len(input_ids))\nprint('Max word length: ', max_len)","metadata":{"id":"XSdd0vuuPuQ3","colab":{"base_uri":"https://localhost:8080/"},"outputId":"be04937c-fbf1-41e4-b7d5-1086baa1da6e","execution":{"iopub.status.busy":"2023-04-27T10:54:15.978360Z","iopub.execute_input":"2023-04-27T10:54:15.979044Z","iopub.status.idle":"2023-04-27T10:56:56.410712Z","shell.execute_reply.started":"2023-04-27T10:54:15.979002Z","shell.execute_reply":"2023-04-27T10:56:56.409353Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Max word length:  58\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom tqdm.notebook import tqdm","metadata":{"id":"iJzUNx__HpPv","execution":{"iopub.status.busy":"2023-04-27T10:56:56.412588Z","iopub.execute_input":"2023-04-27T10:56:56.412989Z","iopub.status.idle":"2023-04-27T10:56:59.000698Z","shell.execute_reply.started":"2023-04-27T10:56:56.412948Z","shell.execute_reply":"2023-04-27T10:56:58.999573Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"\n# Tokenize all of the words and map the tokens to thier word IDs.\ninput_ids = []\nattention_masks = []\nMAX_LENGTH = max_len\n# For every sentence...\nfor word in tqdm(train_words):\n    encoded_dict = tokenizer(\n                        word,                      # Sentence to encode.\n                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                        max_length = MAX_LENGTH,           # Pad & truncate all sentences.\n                        pad_to_max_length = True,\n                        return_attention_mask = True,   # Construct attn. masks.\n                        return_tensors = 'pt',     # Return pytorch tensors.\n                   )\n    \n    input_ids.append(encoded_dict['input_ids'])\n    attention_masks.append(encoded_dict['attention_mask'])\n","metadata":{"id":"eKx2esPocWfb","execution":{"iopub.status.busy":"2023-04-27T10:56:59.002260Z","iopub.execute_input":"2023-04-27T10:56:59.003995Z","iopub.status.idle":"2023-04-27T11:01:52.746910Z","shell.execute_reply.started":"2023-04-27T10:56:59.003950Z","shell.execute_reply":"2023-04-27T11:01:52.745570Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1680027 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba66640677c744e0a89c1415d74aae6d"}},"metadata":{}},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2352: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","output_type":"stream"}]},{"cell_type":"code","source":"# Convert the lists into tensors.\ninput_ids = torch.cat(input_ids, dim=0)\nattention_masks = torch.cat(attention_masks, dim=0)\nlabels = torch.tensor(target.values)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IzEf3ITGsUH1","outputId":"564a2fa9-40ad-459c-f84a-935774042eac","execution":{"iopub.status.busy":"2023-04-27T11:01:52.748550Z","iopub.execute_input":"2023-04-27T11:01:52.749582Z","iopub.status.idle":"2023-04-27T11:02:00.742829Z","shell.execute_reply.started":"2023-04-27T11:01:52.749532Z","shell.execute_reply":"2023-04-27T11:02:00.741763Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"i = 10000\nprint('Original: ', train_words[i])\nprint('Token IDs:', input_ids[i])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KxnSkzSiszkr","outputId":"7ece42be-f5b9-410d-d0f7-fece92e5f288","execution":{"iopub.status.busy":"2023-04-27T11:02:00.744304Z","iopub.execute_input":"2023-04-27T11:02:00.744712Z","iopub.status.idle":"2023-04-27T11:02:00.769045Z","shell.execute_reply.started":"2023-04-27T11:02:00.744667Z","shell.execute_reply":"2023-04-27T11:02:00.767909Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Original:  агарак\nToken IDs: tensor([ 0,  8, 14,  8, 42,  8, 30,  8, 12,  8, 36,  1,  4,  4,  4,  4,  4,  4,\n         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n         4,  4,  4,  4])\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, random_split\nimport pickle\n\nif input() == '0':\n    dataset = TensorDataset(input_ids, attention_masks, labels)\n    with open('dataset.pkl', 'wb') as f:\n        pickle.dump(dataset, f)\nelse:\n    max_len = 58\n    with open('dataset.pkl', 'rb') as f:\n        dataset = pickle.load(f)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TjjGQimsuPoO","outputId":"5d613603-448e-46fe-9f24-d5749ef4e8c1","execution":{"iopub.status.busy":"2023-04-27T11:02:06.807654Z","iopub.execute_input":"2023-04-27T11:02:06.808732Z","iopub.status.idle":"2023-04-27T11:02:15.950926Z","shell.execute_reply.started":"2023-04-27T11:02:06.808692Z","shell.execute_reply":"2023-04-27T11:02:15.949063Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdin","text":" 0\n"}]},{"cell_type":"code","source":"\ntrain_size = int(0.5 * len(dataset))\nval_size = len(dataset) - train_size\n\n# Divide the dataset by randomly selecting samples.\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\nprint('{:>5,} training samples'.format(train_size))\nprint('{:>5,} validation samples'.format(val_size))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hZGqGipTcO4R","outputId":"3c63591b-e2b9-41ce-8dea-b29aa6bd3ae5","execution":{"iopub.status.busy":"2023-04-27T11:02:29.370409Z","iopub.execute_input":"2023-04-27T11:02:29.371038Z","iopub.status.idle":"2023-04-27T11:02:29.524825Z","shell.execute_reply.started":"2023-04-27T11:02:29.371001Z","shell.execute_reply":"2023-04-27T11:02:29.523582Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"840,013 training samples\n840,014 validation samples\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()","metadata":{"id":"rdYv_w3wtwW4","execution":{"iopub.status.busy":"2023-04-27T11:02:31.553965Z","iopub.execute_input":"2023-04-27T11:02:31.554607Z","iopub.status.idle":"2023-04-27T11:02:31.559876Z","shell.execute_reply.started":"2023-04-27T11:02:31.554567Z","shell.execute_reply":"2023-04-27T11:02:31.558645Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n\nbatch_size = 64\n\ntrain_dataloader = DataLoader(\n            train_dataset,  # The training samples.\n            sampler = RandomSampler(train_dataset), # Select batches randomly\n            batch_size = batch_size # Trains with this batch size.\n        )\n\nvalidation_dataloader = DataLoader(\n            val_dataset, # The validation samples.\n            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n            batch_size = batch_size # Evaluate with this batch size.\n        )","metadata":{"id":"iOiv0hMQcO02","execution":{"iopub.status.busy":"2023-04-27T11:02:39.364019Z","iopub.execute_input":"2023-04-27T11:02:39.365010Z","iopub.status.idle":"2023-04-27T11:02:39.373297Z","shell.execute_reply.started":"2023-04-27T11:02:39.364954Z","shell.execute_reply":"2023-04-27T11:02:39.372191Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"### Train Function","metadata":{"id":"eux_bPesjLzU"}},{"cell_type":"code","source":"import time\nimport datetime\nimport numpy as np\n\ndef format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    elapsed_rounded = int(round((elapsed)))\n    return str(datetime.timedelta(seconds=elapsed_rounded))\n\n\n# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","metadata":{"id":"tsgwed4RcOql","execution":{"iopub.status.busy":"2023-04-27T11:02:43.727777Z","iopub.execute_input":"2023-04-27T11:02:43.728145Z","iopub.status.idle":"2023-04-27T11:02:43.735164Z","shell.execute_reply.started":"2023-04-27T11:02:43.728112Z","shell.execute_reply":"2023-04-27T11:02:43.734081Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"import random\n\ndef train_model(model, optimizer, scheduler, epochs, val_step=True):\n    seed_val = 42\n    random.seed(seed_val)\n    np.random.seed(seed_val)\n    torch.manual_seed(seed_val)\n    torch.cuda.manual_seed_all(seed_val)\n\n    training_stats = []\n\n    total_t0 = time.time()\n\n    for epoch_i in range(0, epochs):\n        # train\n        print('\\n======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n        print('Training...')\n\n        t0 = time.time()\n        total_train_loss = 0\n\n        model.train()\n\n        for step, batch in enumerate(train_dataloader):\n            if step % 100 == 0 and not step == 0:\n                elapsed = format_time(time.time() - t0)\n                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n\n            b_input_ids = batch[0].to(device)\n            b_input_mask = batch[1].to(device)\n            b_labels = batch[2].to(device)\n\n            model.zero_grad()        \n\n            output = model(b_input_ids, token_type_ids=None,\n                          attention_mask=b_input_mask, labels=b_labels)\n\n            total_train_loss += output.loss.item()\n            output.loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n\n        avg_train_loss = total_train_loss / len(train_dataloader)            \n        training_time = format_time(time.time() - t0)\n        print(\"\\n  Average training loss: {0:.2f}\".format(avg_train_loss))\n        print(\"  Training epcoh took: {:}\".format(training_time))\n            \n        # val\n        if not val_step:\n            continue\n        print(\"\\nRunning Validation...\")\n\n        t0 = time.time()\n        model.eval()\n\n        total_eval_accuracy = 0\n        total_eval_loss = 0\n        nb_eval_steps = 0\n\n        for batch in validation_dataloader:\n            b_input_ids = batch[0].to(device)\n            b_input_mask = batch[1].to(device)\n            b_labels = batch[2].to(device)\n            \n            with torch.no_grad():        \n                output = model(b_input_ids, token_type_ids=None, \n                              attention_mask=b_input_mask, labels=b_labels)\n                \n            total_eval_loss += output.loss.item()\n\n            logits = output.logits.detach().cpu().numpy()\n            label_ids = b_labels.to('cpu').numpy()\n\n            total_eval_accuracy += flat_accuracy(logits, label_ids)\n            \n\n        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n        print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n\n        avg_val_loss = total_eval_loss / len(validation_dataloader)\n        validation_time = format_time(time.time() - t0)\n        \n        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n        print(\"  Validation took: {:}\".format(validation_time))\n\n        training_stats.append(\n            {\n                'epoch': epoch_i + 1,\n                'Training Loss': avg_train_loss,\n                'Valid. Loss': avg_val_loss,\n                'Valid. Accur.': avg_val_accuracy,\n                'Training Time': training_time,\n                'Validation Time': validation_time\n            }\n        )\n\n    print(\"\")\n    print(\"Training complete!\")\n\n    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))","metadata":{"id":"q3F1KKP9esGe","execution":{"iopub.status.busy":"2023-04-27T11:02:44.448643Z","iopub.execute_input":"2023-04-27T11:02:44.449073Z","iopub.status.idle":"2023-04-27T11:02:44.480916Z","shell.execute_reply.started":"2023-04-27T11:02:44.449039Z","shell.execute_reply":"2023-04-27T11:02:44.479717Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def test_model(model):\n    model.eval()\n    samples = df.sample(10)\n    pred = []\n    with torch.no_grad():\n        for i in range(10):\n            word = samples.iloc[i]['-де']\n            input = torch.tensor(tokenizer(word)['input_ids']).to(device)\n            output = model(input.unsqueeze(0))\n            preds = output.logits.detach().cpu().numpy()\n            ans = np.argmax(preds, axis=1)[0]\n            pred.append(word[:ans] + '^' + word[ans:])\n    samples['prediction'] = pred\n    samples['is_right'] = [int(inp == out) for inp, out in zip(samples['-д^е'], pred)]\n    return samples","metadata":{"id":"G_g2LOK3esB6","execution":{"iopub.status.busy":"2023-04-27T11:02:45.436758Z","iopub.execute_input":"2023-04-27T11:02:45.437362Z","iopub.status.idle":"2023-04-27T11:02:45.445771Z","shell.execute_reply.started":"2023-04-27T11:02:45.437326Z","shell.execute_reply":"2023-04-27T11:02:45.444563Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"### BERT","metadata":{"id":"d_Q_Rf2rjRxU"}},{"cell_type":"code","source":"from transformers import BertForSequenceClassification, AdamW, BertConfig\nfrom transformers import get_linear_schedule_with_warmup\n\n\nmodel = BertForSequenceClassification.from_pretrained(\n    \"cointegrated/rubert-tiny\", \n    num_labels = max_len - 1,   \n    output_attentions = False, # Whether the model returns attentions weights.\n    output_hidden_states = False, # Whether the model returns all hidden-states.\n)\n\nmodel.to(device)\noptimizer = AdamW(model.parameters(),lr = 2e-5, eps = 1e-8)\n\nepochs = 4\ntotal_steps = len(train_dataloader) * epochs\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0, # Default value in run_glue.py\n                                            num_training_steps = total_steps)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":188,"referenced_widgets":["419b97e53a9842f7ae242e4249dc3797","0ee608c218b649a7916fbee284760539","2df73a2904544963a11c1fad9f813740","b94c3bfc73564bb98dc424718cec3a58","f1a37e256c084e0e80afba84994fd1a7","358569e5d3b94e3fb4522a0a35b6084e","2cc58540b2754481b3ac83c137c2ba29","a67a910967024ee7a4eca678696b34c5","43542f8f319c4812b56318ed3e71dbed","bd3c10661cfe4253a7df491f05d2335a","c7a0e6aa4f9b4f898ea2bfdb581e19b2","efaad66443e642b8a781d2a7bdad8220","3e203b1662e64492a2057d4520fa0368","ae85c3386d5d41b1afe0efc511ff94ef","a1f4179ff58042e4bed123c98884ac0d","950d893c4a4e4311a5b1a20aa05b2b60","407368c5a9a746acad253726bd58809a","a31387adc7904ba28185a0fe8c1a78f1","08ea5244434a483696ec055f4df09b54","dc60e7c734c1439caea37c4eaeb81f99","b209c059ebdc4f2495392515d3645f46","7e30d332ebaf4ef7a5c425a859680239"]},"id":"DBBP-Hd2cOxb","outputId":"46feb152-84a5-4742-c333-c0f7cdccfb25"},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/632 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"419b97e53a9842f7ae242e4249dc3797"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/47.7M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efaad66443e642b8a781d2a7bdad8220"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":"Some weights of the model checkpoint at cointegrated/rubert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"}]},{"cell_type":"code","source":"train_model(model, optimizer, scheduler, epochs)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zX2hY5T-esEE","outputId":"0dd671c2-6b9b-4970-8b5f-5bded7bc5190"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":"\n\n======== Epoch 1 / 4 ========\n\nTraining...\n\n  Batch    40  of  3,282.    Elapsed: 0:00:04.\n\n  Batch    80  of  3,282.    Elapsed: 0:00:08.\n\n  Batch   120  of  3,282.    Elapsed: 0:00:11.\n\n  Batch   160  of  3,282.    Elapsed: 0:00:15.\n\n  Batch   200  of  3,282.    Elapsed: 0:00:19.\n\n  Batch   240  of  3,282.    Elapsed: 0:00:23.\n\n  Batch   280  of  3,282.    Elapsed: 0:00:27.\n\n  Batch   320  of  3,282.    Elapsed: 0:00:31.\n\n  Batch   360  of  3,282.    Elapsed: 0:00:35.\n\n  Batch   400  of  3,282.    Elapsed: 0:00:39.\n\n  Batch   440  of  3,282.    Elapsed: 0:00:43.\n\n  Batch   480  of  3,282.    Elapsed: 0:00:46.\n\n  Batch   520  of  3,282.    Elapsed: 0:00:50.\n\n  Batch   560  of  3,282.    Elapsed: 0:00:54.\n\n  Batch   600  of  3,282.    Elapsed: 0:00:58.\n\n  Batch   640  of  3,282.    Elapsed: 0:01:02.\n\n  Batch   680  of  3,282.    Elapsed: 0:01:06.\n\n  Batch   720  of  3,282.    Elapsed: 0:01:10.\n\n  Batch   760  of  3,282.    Elapsed: 0:01:14.\n\n  Batch   800  of  3,282.    Elapsed: 0:01:18.\n\n  Batch   840  of  3,282.    Elapsed: 0:01:22.\n\n  Batch   880  of  3,282.    Elapsed: 0:01:26.\n\n  Batch   920  of  3,282.    Elapsed: 0:01:30.\n\n  Batch   960  of  3,282.    Elapsed: 0:01:34.\n\n  Batch 1,000  of  3,282.    Elapsed: 0:01:38.\n\n  Batch 1,040  of  3,282.    Elapsed: 0:01:42.\n\n  Batch 1,080  of  3,282.    Elapsed: 0:01:46.\n\n  Batch 1,120  of  3,282.    Elapsed: 0:01:50.\n\n  Batch 1,160  of  3,282.    Elapsed: 0:01:54.\n\n  Batch 1,200  of  3,282.    Elapsed: 0:01:58.\n\n  Batch 1,240  of  3,282.    Elapsed: 0:02:02.\n\n  Batch 1,280  of  3,282.    Elapsed: 0:02:06.\n\n  Batch 1,320  of  3,282.    Elapsed: 0:02:10.\n\n  Batch 1,360  of  3,282.    Elapsed: 0:02:14.\n\n  Batch 1,400  of  3,282.    Elapsed: 0:02:18.\n\n  Batch 1,440  of  3,282.    Elapsed: 0:02:22.\n\n  Batch 1,480  of  3,282.    Elapsed: 0:02:26.\n\n  Batch 1,520  of  3,282.    Elapsed: 0:02:30.\n\n  Batch 1,560  of  3,282.    Elapsed: 0:02:34.\n\n  Batch 1,600  of  3,282.    Elapsed: 0:02:38.\n\n  Batch 1,640  of  3,282.    Elapsed: 0:02:43.\n\n  Batch 1,680  of  3,282.    Elapsed: 0:02:47.\n\n  Batch 1,720  of  3,282.    Elapsed: 0:02:51.\n\n  Batch 1,760  of  3,282.    Elapsed: 0:02:55.\n\n  Batch 1,800  of  3,282.    Elapsed: 0:02:59.\n\n  Batch 1,840  of  3,282.    Elapsed: 0:03:03.\n\n  Batch 1,880  of  3,282.    Elapsed: 0:03:07.\n\n  Batch 1,920  of  3,282.    Elapsed: 0:03:11.\n\n  Batch 1,960  of  3,282.    Elapsed: 0:03:15.\n\n  Batch 2,000  of  3,282.    Elapsed: 0:03:19.\n\n  Batch 2,040  of  3,282.    Elapsed: 0:03:24.\n\n  Batch 2,080  of  3,282.    Elapsed: 0:03:28.\n\n  Batch 2,120  of  3,282.    Elapsed: 0:03:32.\n\n  Batch 2,160  of  3,282.    Elapsed: 0:03:36.\n\n  Batch 2,200  of  3,282.    Elapsed: 0:03:40.\n\n  Batch 2,240  of  3,282.    Elapsed: 0:03:44.\n\n  Batch 2,280  of  3,282.    Elapsed: 0:03:48.\n\n  Batch 2,320  of  3,282.    Elapsed: 0:03:52.\n\n  Batch 2,360  of  3,282.    Elapsed: 0:03:56.\n\n  Batch 2,400  of  3,282.    Elapsed: 0:04:00.\n\n  Batch 2,440  of  3,282.    Elapsed: 0:04:05.\n\n  Batch 2,480  of  3,282.    Elapsed: 0:04:09.\n\n  Batch 2,520  of  3,282.    Elapsed: 0:04:13.\n\n  Batch 2,560  of  3,282.    Elapsed: 0:04:17.\n\n  Batch 2,600  of  3,282.    Elapsed: 0:04:21.\n\n  Batch 2,640  of  3,282.    Elapsed: 0:04:25.\n\n  Batch 2,680  of  3,282.    Elapsed: 0:04:30.\n\n  Batch 2,720  of  3,282.    Elapsed: 0:04:34.\n\n  Batch 2,760  of  3,282.    Elapsed: 0:04:38.\n\n  Batch 2,800  of  3,282.    Elapsed: 0:04:42.\n\n  Batch 2,840  of  3,282.    Elapsed: 0:04:46.\n\n  Batch 2,880  of  3,282.    Elapsed: 0:04:50.\n\n  Batch 2,920  of  3,282.    Elapsed: 0:04:54.\n\n  Batch 2,960  of  3,282.    Elapsed: 0:04:59.\n\n  Batch 3,000  of  3,282.    Elapsed: 0:05:03.\n\n  Batch 3,040  of  3,282.    Elapsed: 0:05:07.\n\n  Batch 3,080  of  3,282.    Elapsed: 0:05:11.\n\n  Batch 3,120  of  3,282.    Elapsed: 0:05:15.\n\n  Batch 3,160  of  3,282.    Elapsed: 0:05:19.\n\n  Batch 3,200  of  3,282.    Elapsed: 0:05:24.\n\n  Batch 3,240  of  3,282.    Elapsed: 0:05:28.\n\n  Batch 3,280  of  3,282.    Elapsed: 0:05:32.\n\n\n\n  Average training loss: 1.71\n\n  Training epcoh took: 0:05:32\n\n\n\nRunning Validation...\n\n  Accuracy: 0.64\n\n  Validation Loss: 1.26\n\n  Validation took: 0:02:11\n\n\n\n======== Epoch 2 / 4 ========\n\nTraining...\n\n  Batch    40  of  3,282.    Elapsed: 0:00:04.\n\n  Batch    80  of  3,282.    Elapsed: 0:00:08.\n\n  Batch   120  of  3,282.    Elapsed: 0:00:13.\n\n  Batch   160  of  3,282.    Elapsed: 0:00:17.\n\n  Batch   200  of  3,282.    Elapsed: 0:00:21.\n\n  Batch   240  of  3,282.    Elapsed: 0:00:25.\n\n  Batch   280  of  3,282.    Elapsed: 0:00:29.\n\n  Batch   320  of  3,282.    Elapsed: 0:00:33.\n\n  Batch   360  of  3,282.    Elapsed: 0:00:37.\n\n  Batch   400  of  3,282.    Elapsed: 0:00:42.\n\n  Batch   440  of  3,282.    Elapsed: 0:00:46.\n\n  Batch   480  of  3,282.    Elapsed: 0:00:50.\n\n  Batch   520  of  3,282.    Elapsed: 0:00:54.\n\n  Batch   560  of  3,282.    Elapsed: 0:00:59.\n\n  Batch   600  of  3,282.    Elapsed: 0:01:03.\n\n  Batch   640  of  3,282.    Elapsed: 0:01:07.\n\n  Batch   680  of  3,282.    Elapsed: 0:01:11.\n\n  Batch   720  of  3,282.    Elapsed: 0:01:15.\n\n  Batch   760  of  3,282.    Elapsed: 0:01:19.\n\n  Batch   800  of  3,282.    Elapsed: 0:01:23.\n\n  Batch   840  of  3,282.    Elapsed: 0:01:28.\n\n  Batch   880  of  3,282.    Elapsed: 0:01:32.\n\n  Batch   920  of  3,282.    Elapsed: 0:01:36.\n\n  Batch   960  of  3,282.    Elapsed: 0:01:40.\n\n  Batch 1,000  of  3,282.    Elapsed: 0:01:44.\n\n  Batch 1,040  of  3,282.    Elapsed: 0:01:48.\n\n  Batch 1,080  of  3,282.    Elapsed: 0:01:52.\n\n  Batch 1,120  of  3,282.    Elapsed: 0:01:57.\n\n  Batch 1,160  of  3,282.    Elapsed: 0:02:01.\n\n  Batch 1,200  of  3,282.    Elapsed: 0:02:05.\n\n  Batch 1,240  of  3,282.    Elapsed: 0:02:09.\n\n  Batch 1,280  of  3,282.    Elapsed: 0:02:13.\n\n  Batch 1,320  of  3,282.    Elapsed: 0:02:17.\n\n  Batch 1,360  of  3,282.    Elapsed: 0:02:21.\n\n  Batch 1,400  of  3,282.    Elapsed: 0:02:26.\n\n  Batch 1,440  of  3,282.    Elapsed: 0:02:30.\n\n  Batch 1,480  of  3,282.    Elapsed: 0:02:34.\n\n  Batch 1,520  of  3,282.    Elapsed: 0:02:38.\n\n  Batch 1,560  of  3,282.    Elapsed: 0:02:42.\n\n  Batch 1,600  of  3,282.    Elapsed: 0:02:46.\n\n  Batch 1,640  of  3,282.    Elapsed: 0:02:50.\n\n  Batch 1,680  of  3,282.    Elapsed: 0:02:55.\n\n  Batch 1,720  of  3,282.    Elapsed: 0:02:59.\n\n  Batch 1,760  of  3,282.    Elapsed: 0:03:03.\n\n  Batch 1,800  of  3,282.    Elapsed: 0:03:07.\n\n  Batch 1,840  of  3,282.    Elapsed: 0:03:11.\n\n  Batch 1,880  of  3,282.    Elapsed: 0:03:15.\n\n  Batch 1,920  of  3,282.    Elapsed: 0:03:19.\n\n  Batch 1,960  of  3,282.    Elapsed: 0:03:24.\n\n  Batch 2,000  of  3,282.    Elapsed: 0:03:28.\n\n  Batch 2,040  of  3,282.    Elapsed: 0:03:32.\n\n  Batch 2,080  of  3,282.    Elapsed: 0:03:36.\n\n  Batch 2,120  of  3,282.    Elapsed: 0:03:40.\n\n  Batch 2,160  of  3,282.    Elapsed: 0:03:44.\n\n  Batch 2,200  of  3,282.    Elapsed: 0:03:48.\n\n  Batch 2,240  of  3,282.    Elapsed: 0:03:53.\n\n  Batch 2,280  of  3,282.    Elapsed: 0:03:57.\n\n  Batch 2,320  of  3,282.    Elapsed: 0:04:01.\n\n  Batch 2,360  of  3,282.    Elapsed: 0:04:05.\n\n  Batch 2,400  of  3,282.    Elapsed: 0:04:09.\n\n  Batch 2,440  of  3,282.    Elapsed: 0:04:13.\n\n  Batch 2,480  of  3,282.    Elapsed: 0:04:17.\n\n  Batch 2,520  of  3,282.    Elapsed: 0:04:22.\n\n  Batch 2,560  of  3,282.    Elapsed: 0:04:26.\n\n  Batch 2,600  of  3,282.    Elapsed: 0:04:30.\n\n  Batch 2,640  of  3,282.    Elapsed: 0:04:34.\n\n  Batch 2,680  of  3,282.    Elapsed: 0:04:38.\n\n  Batch 2,720  of  3,282.    Elapsed: 0:04:42.\n\n  Batch 2,760  of  3,282.    Elapsed: 0:04:46.\n\n  Batch 2,800  of  3,282.    Elapsed: 0:04:51.\n\n  Batch 2,840  of  3,282.    Elapsed: 0:04:55.\n\n  Batch 2,880  of  3,282.    Elapsed: 0:04:59.\n\n  Batch 2,920  of  3,282.    Elapsed: 0:05:03.\n\n  Batch 2,960  of  3,282.    Elapsed: 0:05:08.\n\n  Batch 3,000  of  3,282.    Elapsed: 0:05:12.\n\n  Batch 3,040  of  3,282.    Elapsed: 0:05:16.\n\n  Batch 3,080  of  3,282.    Elapsed: 0:05:20.\n\n  Batch 3,120  of  3,282.    Elapsed: 0:05:24.\n\n  Batch 3,160  of  3,282.    Elapsed: 0:05:28.\n\n  Batch 3,200  of  3,282.    Elapsed: 0:05:32.\n\n  Batch 3,240  of  3,282.    Elapsed: 0:05:37.\n\n  Batch 3,280  of  3,282.    Elapsed: 0:05:41.\n\n\n\n  Average training loss: 1.08\n\n  Training epcoh took: 0:05:41\n\n\n\nRunning Validation...\n\n  Accuracy: 0.76\n\n  Validation Loss: 0.76\n\n  Validation took: 0:02:12\n\n\n\n======== Epoch 3 / 4 ========\n\nTraining...\n\n  Batch    40  of  3,282.    Elapsed: 0:00:04.\n\n  Batch    80  of  3,282.    Elapsed: 0:00:08.\n\n  Batch   120  of  3,282.    Elapsed: 0:00:13.\n\n  Batch   160  of  3,282.    Elapsed: 0:00:17.\n\n  Batch   200  of  3,282.    Elapsed: 0:00:21.\n\n  Batch   240  of  3,282.    Elapsed: 0:00:25.\n\n  Batch   280  of  3,282.    Elapsed: 0:00:29.\n\n  Batch   320  of  3,282.    Elapsed: 0:00:33.\n\n  Batch   360  of  3,282.    Elapsed: 0:00:37.\n\n  Batch   400  of  3,282.    Elapsed: 0:00:42.\n\n  Batch   440  of  3,282.    Elapsed: 0:00:46.\n\n  Batch   480  of  3,282.    Elapsed: 0:00:50.\n\n  Batch   520  of  3,282.    Elapsed: 0:00:54.\n\n  Batch   560  of  3,282.    Elapsed: 0:00:58.\n\n  Batch   600  of  3,282.    Elapsed: 0:01:02.\n\n  Batch   640  of  3,282.    Elapsed: 0:01:07.\n\n  Batch   680  of  3,282.    Elapsed: 0:01:11.\n\n  Batch   720  of  3,282.    Elapsed: 0:01:15.\n\n  Batch   760  of  3,282.    Elapsed: 0:01:19.\n\n  Batch   800  of  3,282.    Elapsed: 0:01:23.\n\n  Batch   840  of  3,282.    Elapsed: 0:01:28.\n\n  Batch   880  of  3,282.    Elapsed: 0:01:32.\n\n  Batch   920  of  3,282.    Elapsed: 0:01:37.\n\n  Batch   960  of  3,282.    Elapsed: 0:01:41.\n\n  Batch 1,000  of  3,282.    Elapsed: 0:01:45.\n\n  Batch 1,040  of  3,282.    Elapsed: 0:01:49.\n\n  Batch 1,080  of  3,282.    Elapsed: 0:01:53.\n\n  Batch 1,120  of  3,282.    Elapsed: 0:01:58.\n\n  Batch 1,160  of  3,282.    Elapsed: 0:02:02.\n\n  Batch 1,200  of  3,282.    Elapsed: 0:02:06.\n\n  Batch 1,240  of  3,282.    Elapsed: 0:02:11.\n\n  Batch 1,280  of  3,282.    Elapsed: 0:02:15.\n\n  Batch 1,320  of  3,282.    Elapsed: 0:02:19.\n\n  Batch 1,360  of  3,282.    Elapsed: 0:02:23.\n\n  Batch 1,400  of  3,282.    Elapsed: 0:02:27.\n\n  Batch 1,440  of  3,282.    Elapsed: 0:02:32.\n\n  Batch 1,480  of  3,282.    Elapsed: 0:02:36.\n\n  Batch 1,520  of  3,282.    Elapsed: 0:02:41.\n\n  Batch 1,560  of  3,282.    Elapsed: 0:02:45.\n\n  Batch 1,600  of  3,282.    Elapsed: 0:02:49.\n\n  Batch 1,640  of  3,282.    Elapsed: 0:02:53.\n\n  Batch 1,680  of  3,282.    Elapsed: 0:02:58.\n\n  Batch 1,720  of  3,282.    Elapsed: 0:03:02.\n\n  Batch 1,760  of  3,282.    Elapsed: 0:03:06.\n\n  Batch 1,800  of  3,282.    Elapsed: 0:03:10.\n\n  Batch 1,840  of  3,282.    Elapsed: 0:03:14.\n\n  Batch 1,880  of  3,282.    Elapsed: 0:03:18.\n\n  Batch 1,920  of  3,282.    Elapsed: 0:03:23.\n\n  Batch 1,960  of  3,282.    Elapsed: 0:03:28.\n\n  Batch 2,000  of  3,282.    Elapsed: 0:03:32.\n\n  Batch 2,040  of  3,282.    Elapsed: 0:03:36.\n\n  Batch 2,080  of  3,282.    Elapsed: 0:03:40.\n\n  Batch 2,120  of  3,282.    Elapsed: 0:03:45.\n\n  Batch 2,160  of  3,282.    Elapsed: 0:03:49.\n\n  Batch 2,200  of  3,282.    Elapsed: 0:03:53.\n\n  Batch 2,240  of  3,282.    Elapsed: 0:03:58.\n\n  Batch 2,280  of  3,282.    Elapsed: 0:04:02.\n\n  Batch 2,320  of  3,282.    Elapsed: 0:04:06.\n\n  Batch 2,360  of  3,282.    Elapsed: 0:04:11.\n\n  Batch 2,400  of  3,282.    Elapsed: 0:04:15.\n\n  Batch 2,440  of  3,282.    Elapsed: 0:04:19.\n\n  Batch 2,480  of  3,282.    Elapsed: 0:04:23.\n\n  Batch 2,520  of  3,282.    Elapsed: 0:04:28.\n\n  Batch 2,560  of  3,282.    Elapsed: 0:04:32.\n\n  Batch 2,600  of  3,282.    Elapsed: 0:04:36.\n\n  Batch 2,640  of  3,282.    Elapsed: 0:04:41.\n\n  Batch 2,680  of  3,282.    Elapsed: 0:04:45.\n\n  Batch 2,720  of  3,282.    Elapsed: 0:04:49.\n\n  Batch 2,760  of  3,282.    Elapsed: 0:04:53.\n\n  Batch 2,800  of  3,282.    Elapsed: 0:04:57.\n\n  Batch 2,840  of  3,282.    Elapsed: 0:05:02.\n\n  Batch 2,880  of  3,282.    Elapsed: 0:05:06.\n\n  Batch 2,920  of  3,282.    Elapsed: 0:05:11.\n\n  Batch 2,960  of  3,282.    Elapsed: 0:05:15.\n\n  Batch 3,000  of  3,282.    Elapsed: 0:05:20.\n\n  Batch 3,040  of  3,282.    Elapsed: 0:05:24.\n\n  Batch 3,080  of  3,282.    Elapsed: 0:05:28.\n\n  Batch 3,120  of  3,282.    Elapsed: 0:05:32.\n\n  Batch 3,160  of  3,282.    Elapsed: 0:05:37.\n\n  Batch 3,200  of  3,282.    Elapsed: 0:05:41.\n\n  Batch 3,240  of  3,282.    Elapsed: 0:05:46.\n\n  Batch 3,280  of  3,282.    Elapsed: 0:05:50.\n\n\n\n  Average training loss: 0.77\n\n  Training epcoh took: 0:05:50\n\n\n\nRunning Validation...\n\n  Accuracy: 0.78\n\n  Validation Loss: 0.65\n\n  Validation took: 0:02:14\n\n\n\n======== Epoch 4 / 4 ========\n\nTraining...\n\n  Batch    40  of  3,282.    Elapsed: 0:00:04.\n\n  Batch    80  of  3,282.    Elapsed: 0:00:08.\n\n  Batch   120  of  3,282.    Elapsed: 0:00:13.\n\n  Batch   160  of  3,282.    Elapsed: 0:00:17.\n\n  Batch   200  of  3,282.    Elapsed: 0:00:21.\n\n  Batch   240  of  3,282.    Elapsed: 0:00:25.\n\n  Batch   280  of  3,282.    Elapsed: 0:00:29.\n\n  Batch   320  of  3,282.    Elapsed: 0:00:33.\n\n  Batch   360  of  3,282.    Elapsed: 0:00:37.\n\n  Batch   400  of  3,282.    Elapsed: 0:00:42.\n\n  Batch   440  of  3,282.    Elapsed: 0:00:46.\n\n  Batch   480  of  3,282.    Elapsed: 0:00:50.\n\n  Batch   520  of  3,282.    Elapsed: 0:00:54.\n\n  Batch   560  of  3,282.    Elapsed: 0:00:58.\n\n  Batch   600  of  3,282.    Elapsed: 0:01:02.\n\n  Batch   640  of  3,282.    Elapsed: 0:01:07.\n\n  Batch   680  of  3,282.    Elapsed: 0:01:11.\n\n  Batch   720  of  3,282.    Elapsed: 0:01:16.\n\n  Batch   760  of  3,282.    Elapsed: 0:01:20.\n\n  Batch   800  of  3,282.    Elapsed: 0:01:24.\n\n  Batch   840  of  3,282.    Elapsed: 0:01:28.\n\n  Batch   880  of  3,282.    Elapsed: 0:01:33.\n\n  Batch   920  of  3,282.    Elapsed: 0:01:37.\n\n  Batch   960  of  3,282.    Elapsed: 0:01:41.\n\n  Batch 1,000  of  3,282.    Elapsed: 0:01:45.\n\n  Batch 1,040  of  3,282.    Elapsed: 0:01:49.\n\n  Batch 1,080  of  3,282.    Elapsed: 0:01:54.\n\n  Batch 1,120  of  3,282.    Elapsed: 0:01:58.\n\n  Batch 1,160  of  3,282.    Elapsed: 0:02:02.\n\n  Batch 1,200  of  3,282.    Elapsed: 0:02:06.\n\n  Batch 1,240  of  3,282.    Elapsed: 0:02:11.\n\n  Batch 1,280  of  3,282.    Elapsed: 0:02:15.\n\n  Batch 1,320  of  3,282.    Elapsed: 0:02:19.\n\n  Batch 1,360  of  3,282.    Elapsed: 0:02:23.\n\n  Batch 1,400  of  3,282.    Elapsed: 0:02:27.\n\n  Batch 1,440  of  3,282.    Elapsed: 0:02:31.\n\n  Batch 1,480  of  3,282.    Elapsed: 0:02:35.\n\n  Batch 1,520  of  3,282.    Elapsed: 0:02:39.\n\n  Batch 1,560  of  3,282.    Elapsed: 0:02:44.\n\n  Batch 1,600  of  3,282.    Elapsed: 0:02:48.\n\n  Batch 1,640  of  3,282.    Elapsed: 0:02:52.\n\n  Batch 1,680  of  3,282.    Elapsed: 0:02:56.\n\n  Batch 1,720  of  3,282.    Elapsed: 0:03:00.\n\n  Batch 1,760  of  3,282.    Elapsed: 0:03:04.\n\n  Batch 1,800  of  3,282.    Elapsed: 0:03:08.\n\n  Batch 1,840  of  3,282.    Elapsed: 0:03:13.\n\n  Batch 1,880  of  3,282.    Elapsed: 0:03:17.\n\n  Batch 1,920  of  3,282.    Elapsed: 0:03:21.\n\n  Batch 1,960  of  3,282.    Elapsed: 0:03:25.\n\n  Batch 2,000  of  3,282.    Elapsed: 0:03:29.\n\n  Batch 2,040  of  3,282.    Elapsed: 0:03:33.\n\n  Batch 2,080  of  3,282.    Elapsed: 0:03:37.\n\n  Batch 2,120  of  3,282.    Elapsed: 0:03:41.\n\n  Batch 2,160  of  3,282.    Elapsed: 0:03:46.\n\n  Batch 2,200  of  3,282.    Elapsed: 0:03:50.\n\n  Batch 2,240  of  3,282.    Elapsed: 0:03:54.\n\n  Batch 2,280  of  3,282.    Elapsed: 0:03:58.\n\n  Batch 2,320  of  3,282.    Elapsed: 0:04:02.\n\n  Batch 2,360  of  3,282.    Elapsed: 0:04:06.\n\n  Batch 2,400  of  3,282.    Elapsed: 0:04:10.\n\n  Batch 2,440  of  3,282.    Elapsed: 0:04:15.\n\n  Batch 2,480  of  3,282.    Elapsed: 0:04:19.\n\n  Batch 2,520  of  3,282.    Elapsed: 0:04:23.\n\n  Batch 2,560  of  3,282.    Elapsed: 0:04:27.\n\n  Batch 2,600  of  3,282.    Elapsed: 0:04:31.\n\n  Batch 2,640  of  3,282.    Elapsed: 0:04:35.\n\n  Batch 2,680  of  3,282.    Elapsed: 0:04:39.\n\n  Batch 2,720  of  3,282.    Elapsed: 0:04:44.\n\n  Batch 2,760  of  3,282.    Elapsed: 0:04:48.\n\n  Batch 2,800  of  3,282.    Elapsed: 0:04:52.\n\n  Batch 2,840  of  3,282.    Elapsed: 0:04:56.\n\n  Batch 2,880  of  3,282.    Elapsed: 0:05:00.\n\n  Batch 2,920  of  3,282.    Elapsed: 0:05:04.\n\n  Batch 2,960  of  3,282.    Elapsed: 0:05:08.\n\n  Batch 3,000  of  3,282.    Elapsed: 0:05:13.\n\n  Batch 3,040  of  3,282.    Elapsed: 0:05:17.\n\n  Batch 3,080  of  3,282.    Elapsed: 0:05:21.\n\n  Batch 3,120  of  3,282.    Elapsed: 0:05:25.\n\n  Batch 3,160  of  3,282.    Elapsed: 0:05:29.\n\n  Batch 3,200  of  3,282.    Elapsed: 0:05:33.\n\n  Batch 3,240  of  3,282.    Elapsed: 0:05:38.\n\n  Batch 3,280  of  3,282.    Elapsed: 0:05:42.\n\n\n\n  Average training loss: 0.71\n\n  Training epcoh took: 0:05:42\n\n\n\nRunning Validation...\n\n  Accuracy: 0.79\n\n  Validation Loss: 0.63\n\n  Validation took: 0:02:12\n\n\n\nTraining complete!\n\nTotal training took 0:31:34 (h:mm:ss)\n"}]},{"cell_type":"markdown","source":"accuracy: 0.79","metadata":{"id":"7wOG_-X57zG_"}},{"cell_type":"code","source":"torch.save(model.state_dict(), 'rubert_weights.pth')","metadata":{"id":"2IF2PfHa8ma-"},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"model.load_state_dict(torch.load('rubert_weights.pth'))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"obCPRz8TAu7W","outputId":"8dd10ce8-fef3-4293-8687-17f3b6957c30"},"execution_count":51,"outputs":[{"output_type":"execute_result","execution_count":51,"data":{"text/plain":["<All keys matched successfully>"]},"metadata":{}}]},{"cell_type":"code","source":"test_model(model)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"id":"lkSUl53Ver_Q","outputId":"2915977c-a475-436f-8af6-d0b9d469f742"},"execution_count":49,"outputs":[{"output_type":"execute_result","execution_count":49,"data":{"text/plain":["                    -де             -д^е  target_idx       prediction  \\\n","177319    возродившеюся   возрод^ившеюся           6   возрод^ившеюся   \n","293072           гурвиц          г^урвиц           1          г^урвиц   \n","1294652    рассердивший    рассерд^ивший           7    рассерд^ивший   \n","141466        верталось       верт^алось           4       верт^алось   \n","1600328        хлипкому        хл^ипкому           2        хл^ипкому   \n","1002135  перфораторного  перфор^аторного           6  перфор^аторного   \n","311153      дергавшиеся     д^ергавшиеся           1     дерг^авшиеся   \n","606041   лавинообразную  лавинообр^азную           9  лавинообр^азную   \n","663080          месящей         мес^ящей           3         мес^ящей   \n","1623480      черствеешь      черств^еешь           6      черств^еешь   \n","\n","         is_right  \n","177319          1  \n","293072          1  \n","1294652         1  \n","141466          1  \n","1600328         1  \n","1002135         1  \n","311153          0  \n","606041          1  \n","663080          1  \n","1623480         1  "],"text/html":["\n","  <div id=\"df-4816c7ba-8a2e-4789-8207-ad6a680efc50\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>-де</th>\n","      <th>-д^е</th>\n","      <th>target_idx</th>\n","      <th>prediction</th>\n","      <th>is_right</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>177319</th>\n","      <td>возродившеюся</td>\n","      <td>возрод^ившеюся</td>\n","      <td>6</td>\n","      <td>возрод^ившеюся</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>293072</th>\n","      <td>гурвиц</td>\n","      <td>г^урвиц</td>\n","      <td>1</td>\n","      <td>г^урвиц</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1294652</th>\n","      <td>рассердивший</td>\n","      <td>рассерд^ивший</td>\n","      <td>7</td>\n","      <td>рассерд^ивший</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>141466</th>\n","      <td>верталось</td>\n","      <td>верт^алось</td>\n","      <td>4</td>\n","      <td>верт^алось</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1600328</th>\n","      <td>хлипкому</td>\n","      <td>хл^ипкому</td>\n","      <td>2</td>\n","      <td>хл^ипкому</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1002135</th>\n","      <td>перфораторного</td>\n","      <td>перфор^аторного</td>\n","      <td>6</td>\n","      <td>перфор^аторного</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>311153</th>\n","      <td>дергавшиеся</td>\n","      <td>д^ергавшиеся</td>\n","      <td>1</td>\n","      <td>дерг^авшиеся</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>606041</th>\n","      <td>лавинообразную</td>\n","      <td>лавинообр^азную</td>\n","      <td>9</td>\n","      <td>лавинообр^азную</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>663080</th>\n","      <td>месящей</td>\n","      <td>мес^ящей</td>\n","      <td>3</td>\n","      <td>мес^ящей</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1623480</th>\n","      <td>черствеешь</td>\n","      <td>черств^еешь</td>\n","      <td>6</td>\n","      <td>черств^еешь</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4816c7ba-8a2e-4789-8207-ad6a680efc50')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-4816c7ba-8a2e-4789-8207-ad6a680efc50 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-4816c7ba-8a2e-4789-8207-ad6a680efc50');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}}]},{"cell_type":"markdown","source":"### Albert","metadata":{"id":"wcS_w_3QC50x"}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AdamW\nfrom transformers import get_linear_schedule_with_warmup\n\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"albert-base-v2\", \n    num_labels = max_len - 1,   \n    output_attentions = False, # Whether the model returns attentions weights.\n    output_hidden_states = False, # Whether the model returns all hidden-states.\n)\n\nmodel.to(device)\noptimizer = AdamW(model.parameters(),lr = 2e-5, eps = 1e-8)\n\nepochs = 1\ntotal_steps = len(train_dataloader) * epochs\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0, # Default value in run_glue.py\n                                            num_training_steps = total_steps)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FPWwFEIOC4lB","outputId":"bd362b3d-36d5-42d2-e40f-e8d5685bbe1c","execution":{"iopub.status.busy":"2023-04-27T11:03:02.549521Z","iopub.execute_input":"2023-04-27T11:03:02.550558Z","iopub.status.idle":"2023-04-27T11:03:18.132562Z","shell.execute_reply.started":"2023-04-27T11:03:02.550508Z","shell.execute_reply":"2023-04-27T11:03:18.130465Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cbdf8258ae94b55b78bb2e79809fe38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/47.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dd9756717aa4abfacb11f44ca2c21c5"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias']\n- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:395: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"id":"44HJl-HiIl6_","execution":{"iopub.status.busy":"2023-04-27T11:03:18.136278Z","iopub.execute_input":"2023-04-27T11:03:18.136673Z","iopub.status.idle":"2023-04-27T11:03:18.143234Z","shell.execute_reply.started":"2023-04-27T11:03:18.136620Z","shell.execute_reply":"2023-04-27T11:03:18.142039Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"train_model(model, optimizer, scheduler, epochs, val_step=False)","metadata":{"id":"xYkIhwDfAu0e","execution":{"iopub.status.busy":"2023-04-27T11:03:20.818353Z","iopub.execute_input":"2023-04-27T11:03:20.818946Z","iopub.status.idle":"2023-04-27T13:17:16.624904Z","shell.execute_reply.started":"2023-04-27T11:03:20.818904Z","shell.execute_reply":"2023-04-27T13:17:16.623643Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"\n======== Epoch 1 / 1 ========\nTraining...\n  Batch   100  of  13,126.    Elapsed: 0:00:59.\n  Batch   200  of  13,126.    Elapsed: 0:01:58.\n  Batch   300  of  13,126.    Elapsed: 0:03:00.\n  Batch   400  of  13,126.    Elapsed: 0:04:00.\n  Batch   500  of  13,126.    Elapsed: 0:05:02.\n  Batch   600  of  13,126.    Elapsed: 0:06:03.\n  Batch   700  of  13,126.    Elapsed: 0:07:04.\n  Batch   800  of  13,126.    Elapsed: 0:08:05.\n  Batch   900  of  13,126.    Elapsed: 0:09:06.\n  Batch 1,000  of  13,126.    Elapsed: 0:10:07.\n  Batch 1,100  of  13,126.    Elapsed: 0:11:08.\n  Batch 1,200  of  13,126.    Elapsed: 0:12:09.\n  Batch 1,300  of  13,126.    Elapsed: 0:13:10.\n  Batch 1,400  of  13,126.    Elapsed: 0:14:12.\n  Batch 1,500  of  13,126.    Elapsed: 0:15:13.\n  Batch 1,600  of  13,126.    Elapsed: 0:16:14.\n  Batch 1,700  of  13,126.    Elapsed: 0:17:15.\n  Batch 1,800  of  13,126.    Elapsed: 0:18:16.\n  Batch 1,900  of  13,126.    Elapsed: 0:19:18.\n  Batch 2,000  of  13,126.    Elapsed: 0:20:19.\n  Batch 2,100  of  13,126.    Elapsed: 0:21:20.\n  Batch 2,200  of  13,126.    Elapsed: 0:22:21.\n  Batch 2,300  of  13,126.    Elapsed: 0:23:22.\n  Batch 2,400  of  13,126.    Elapsed: 0:24:23.\n  Batch 2,500  of  13,126.    Elapsed: 0:25:25.\n  Batch 2,600  of  13,126.    Elapsed: 0:26:26.\n  Batch 2,700  of  13,126.    Elapsed: 0:27:27.\n  Batch 2,800  of  13,126.    Elapsed: 0:28:28.\n  Batch 2,900  of  13,126.    Elapsed: 0:29:29.\n  Batch 3,000  of  13,126.    Elapsed: 0:30:30.\n  Batch 3,100  of  13,126.    Elapsed: 0:31:32.\n  Batch 3,200  of  13,126.    Elapsed: 0:32:33.\n  Batch 3,300  of  13,126.    Elapsed: 0:33:34.\n  Batch 3,400  of  13,126.    Elapsed: 0:34:35.\n  Batch 3,500  of  13,126.    Elapsed: 0:35:37.\n  Batch 3,600  of  13,126.    Elapsed: 0:36:38.\n  Batch 3,700  of  13,126.    Elapsed: 0:37:39.\n  Batch 3,800  of  13,126.    Elapsed: 0:38:40.\n  Batch 3,900  of  13,126.    Elapsed: 0:39:42.\n  Batch 4,000  of  13,126.    Elapsed: 0:40:43.\n  Batch 4,100  of  13,126.    Elapsed: 0:41:44.\n  Batch 4,200  of  13,126.    Elapsed: 0:42:45.\n  Batch 4,300  of  13,126.    Elapsed: 0:43:47.\n  Batch 4,400  of  13,126.    Elapsed: 0:44:48.\n  Batch 4,500  of  13,126.    Elapsed: 0:45:49.\n  Batch 4,600  of  13,126.    Elapsed: 0:46:50.\n  Batch 4,700  of  13,126.    Elapsed: 0:47:51.\n  Batch 4,800  of  13,126.    Elapsed: 0:48:53.\n  Batch 4,900  of  13,126.    Elapsed: 0:49:54.\n  Batch 5,000  of  13,126.    Elapsed: 0:50:55.\n  Batch 5,100  of  13,126.    Elapsed: 0:51:56.\n  Batch 5,200  of  13,126.    Elapsed: 0:52:58.\n  Batch 5,300  of  13,126.    Elapsed: 0:53:59.\n  Batch 5,400  of  13,126.    Elapsed: 0:55:00.\n  Batch 5,500  of  13,126.    Elapsed: 0:56:02.\n  Batch 5,600  of  13,126.    Elapsed: 0:57:03.\n  Batch 5,700  of  13,126.    Elapsed: 0:58:04.\n  Batch 5,800  of  13,126.    Elapsed: 0:59:05.\n  Batch 5,900  of  13,126.    Elapsed: 1:00:07.\n  Batch 6,000  of  13,126.    Elapsed: 1:01:08.\n  Batch 6,100  of  13,126.    Elapsed: 1:02:09.\n  Batch 6,200  of  13,126.    Elapsed: 1:03:10.\n  Batch 6,300  of  13,126.    Elapsed: 1:04:12.\n  Batch 6,400  of  13,126.    Elapsed: 1:05:13.\n  Batch 6,500  of  13,126.    Elapsed: 1:06:14.\n  Batch 6,600  of  13,126.    Elapsed: 1:07:15.\n  Batch 6,700  of  13,126.    Elapsed: 1:08:17.\n  Batch 6,800  of  13,126.    Elapsed: 1:09:18.\n  Batch 6,900  of  13,126.    Elapsed: 1:10:19.\n  Batch 7,000  of  13,126.    Elapsed: 1:11:20.\n  Batch 7,100  of  13,126.    Elapsed: 1:12:22.\n  Batch 7,200  of  13,126.    Elapsed: 1:13:23.\n  Batch 7,300  of  13,126.    Elapsed: 1:14:24.\n  Batch 7,400  of  13,126.    Elapsed: 1:15:25.\n  Batch 7,500  of  13,126.    Elapsed: 1:16:27.\n  Batch 7,600  of  13,126.    Elapsed: 1:17:28.\n  Batch 7,700  of  13,126.    Elapsed: 1:18:29.\n  Batch 7,800  of  13,126.    Elapsed: 1:19:31.\n  Batch 7,900  of  13,126.    Elapsed: 1:20:32.\n  Batch 8,000  of  13,126.    Elapsed: 1:21:33.\n  Batch 8,100  of  13,126.    Elapsed: 1:22:35.\n  Batch 8,200  of  13,126.    Elapsed: 1:23:36.\n  Batch 8,300  of  13,126.    Elapsed: 1:24:37.\n  Batch 8,400  of  13,126.    Elapsed: 1:25:39.\n  Batch 8,500  of  13,126.    Elapsed: 1:26:40.\n  Batch 8,600  of  13,126.    Elapsed: 1:27:41.\n  Batch 8,700  of  13,126.    Elapsed: 1:28:43.\n  Batch 8,800  of  13,126.    Elapsed: 1:29:44.\n  Batch 8,900  of  13,126.    Elapsed: 1:30:45.\n  Batch 9,000  of  13,126.    Elapsed: 1:31:47.\n  Batch 9,100  of  13,126.    Elapsed: 1:32:48.\n  Batch 9,200  of  13,126.    Elapsed: 1:33:49.\n  Batch 9,300  of  13,126.    Elapsed: 1:34:51.\n  Batch 9,400  of  13,126.    Elapsed: 1:35:52.\n  Batch 9,500  of  13,126.    Elapsed: 1:36:54.\n  Batch 9,600  of  13,126.    Elapsed: 1:37:55.\n  Batch 9,700  of  13,126.    Elapsed: 1:38:56.\n  Batch 9,800  of  13,126.    Elapsed: 1:39:58.\n  Batch 9,900  of  13,126.    Elapsed: 1:40:59.\n  Batch 10,000  of  13,126.    Elapsed: 1:42:00.\n  Batch 10,100  of  13,126.    Elapsed: 1:43:01.\n  Batch 10,200  of  13,126.    Elapsed: 1:44:03.\n  Batch 10,300  of  13,126.    Elapsed: 1:45:04.\n  Batch 10,400  of  13,126.    Elapsed: 1:46:05.\n  Batch 10,500  of  13,126.    Elapsed: 1:47:07.\n  Batch 10,600  of  13,126.    Elapsed: 1:48:08.\n  Batch 10,700  of  13,126.    Elapsed: 1:49:09.\n  Batch 10,800  of  13,126.    Elapsed: 1:50:10.\n  Batch 10,900  of  13,126.    Elapsed: 1:51:12.\n  Batch 11,000  of  13,126.    Elapsed: 1:52:13.\n  Batch 11,100  of  13,126.    Elapsed: 1:53:14.\n  Batch 11,200  of  13,126.    Elapsed: 1:54:16.\n  Batch 11,300  of  13,126.    Elapsed: 1:55:17.\n  Batch 11,400  of  13,126.    Elapsed: 1:56:18.\n  Batch 11,500  of  13,126.    Elapsed: 1:57:20.\n  Batch 11,600  of  13,126.    Elapsed: 1:58:21.\n  Batch 11,700  of  13,126.    Elapsed: 1:59:23.\n  Batch 11,800  of  13,126.    Elapsed: 2:00:24.\n  Batch 11,900  of  13,126.    Elapsed: 2:01:25.\n  Batch 12,000  of  13,126.    Elapsed: 2:02:26.\n  Batch 12,100  of  13,126.    Elapsed: 2:03:28.\n  Batch 12,200  of  13,126.    Elapsed: 2:04:29.\n  Batch 12,300  of  13,126.    Elapsed: 2:05:30.\n  Batch 12,400  of  13,126.    Elapsed: 2:06:32.\n  Batch 12,500  of  13,126.    Elapsed: 2:07:33.\n  Batch 12,600  of  13,126.    Elapsed: 2:08:34.\n  Batch 12,700  of  13,126.    Elapsed: 2:09:35.\n  Batch 12,800  of  13,126.    Elapsed: 2:10:37.\n  Batch 12,900  of  13,126.    Elapsed: 2:11:38.\n  Batch 13,000  of  13,126.    Elapsed: 2:12:39.\n  Batch 13,100  of  13,126.    Elapsed: 2:13:40.\n\n  Average training loss: 0.79\n  Training epcoh took: 2:13:56\n\nTraining complete!\nTotal training took 2:13:56 (h:mm:ss)\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'albert_weights.pth')","metadata":{"id":"KcfI0_ETAux8","execution":{"iopub.status.busy":"2023-04-27T13:17:16.627582Z","iopub.execute_input":"2023-04-27T13:17:16.628302Z","iopub.status.idle":"2023-04-27T13:17:16.755132Z","shell.execute_reply.started":"2023-04-27T13:17:16.628258Z","shell.execute_reply":"2023-04-27T13:17:16.753989Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"model.load_state_dict(torch.load('albert_weights.pth'))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FDf-rL5MAuuw","outputId":"b003c202-654b-4222-b583-43c3d7fcf54f","execution":{"iopub.status.busy":"2023-04-27T13:17:16.756552Z","iopub.execute_input":"2023-04-27T13:17:16.756936Z","iopub.status.idle":"2023-04-27T13:17:16.798570Z","shell.execute_reply.started":"2023-04-27T13:17:16.756893Z","shell.execute_reply":"2023-04-27T13:17:16.797553Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"test_model(model)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"id":"U7F9cga4HCEd","outputId":"08c14f5d-6016-4e64-a77e-babc9423a02c","execution":{"iopub.status.busy":"2023-04-27T13:17:16.801269Z","iopub.execute_input":"2023-04-27T13:17:16.801724Z","iopub.status.idle":"2023-04-27T13:17:16.999253Z","shell.execute_reply.started":"2023-04-27T13:17:16.801683Z","shell.execute_reply":"2023-04-27T13:17:16.998191Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"                   -де            -д^е  target_idx      prediction  is_right\n331232   додумывавшими  дод^умывавшими           3  дод^умывавшими         1\n647800        маринова       мар^инова           3       мар^инова         1\n431832    зарядившимся   заряд^ившимся           5   заряд^ившимся         1\n424960      заполучаем     заполуч^аем           7     заполуч^аем         1\n1643854          шлихи          шлих^и           4          шл^ихи         0\n442768        затечешь       затеч^ешь           5       затеч^ешь         1\n563471     комплементе    комплем^енте           7    комплем^енте         1\n819140    обмельчавшей   обмельч^авшей           7   обмельч^авшей         1\n1653022       щурившим       щ^урившим           1       щ^урившим         1\n1363772    сдружаетесь    сдруж^аетесь           5    сдруж^аетесь         1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>-де</th>\n      <th>-д^е</th>\n      <th>target_idx</th>\n      <th>prediction</th>\n      <th>is_right</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>331232</th>\n      <td>додумывавшими</td>\n      <td>дод^умывавшими</td>\n      <td>3</td>\n      <td>дод^умывавшими</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>647800</th>\n      <td>маринова</td>\n      <td>мар^инова</td>\n      <td>3</td>\n      <td>мар^инова</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>431832</th>\n      <td>зарядившимся</td>\n      <td>заряд^ившимся</td>\n      <td>5</td>\n      <td>заряд^ившимся</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>424960</th>\n      <td>заполучаем</td>\n      <td>заполуч^аем</td>\n      <td>7</td>\n      <td>заполуч^аем</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1643854</th>\n      <td>шлихи</td>\n      <td>шлих^и</td>\n      <td>4</td>\n      <td>шл^ихи</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>442768</th>\n      <td>затечешь</td>\n      <td>затеч^ешь</td>\n      <td>5</td>\n      <td>затеч^ешь</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>563471</th>\n      <td>комплементе</td>\n      <td>комплем^енте</td>\n      <td>7</td>\n      <td>комплем^енте</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>819140</th>\n      <td>обмельчавшей</td>\n      <td>обмельч^авшей</td>\n      <td>7</td>\n      <td>обмельч^авшей</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1653022</th>\n      <td>щурившим</td>\n      <td>щ^урившим</td>\n      <td>1</td>\n      <td>щ^урившим</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1363772</th>\n      <td>сдружаетесь</td>\n      <td>сдруж^аетесь</td>\n      <td>5</td>\n      <td>сдруж^аетесь</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Deberta","metadata":{"id":"QYRVwG3ENt9_"}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AdamW\nfrom transformers import get_linear_schedule_with_warmup\n\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"microsoft/deberta-v3-small\", \n    num_labels = max_len - 1,   \n    output_attentions = False, # Whether the model returns attentions weights.\n    output_hidden_states = False, # Whether the model returns all hidden-states.\n)\n\nmodel.to(device)\noptimizer = AdamW(model.parameters(),lr = 2e-5, eps = 1e-8)\n\nepochs = 1\ntotal_steps = len(train_dataloader) * epochs\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0, # Default value in run_glue.py\n                                            num_training_steps = total_steps)\n","metadata":{"id":"MEfjrwZZHCCu","execution":{"iopub.status.busy":"2023-04-27T13:47:10.682484Z","iopub.execute_input":"2023-04-27T13:47:10.683487Z","iopub.status.idle":"2023-04-27T13:47:12.201922Z","shell.execute_reply.started":"2023-04-27T13:47:10.683440Z","shell.execute_reply":"2023-04-27T13:47:12.200902Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight']\n- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"train_model(model, optimizer, scheduler, epochs, val_step=False)","metadata":{"id":"DoTWbzFIHB_P","execution":{"iopub.status.busy":"2023-04-27T13:47:15.875121Z","iopub.execute_input":"2023-04-27T13:47:15.876219Z","iopub.status.idle":"2023-04-27T15:17:54.845829Z","shell.execute_reply.started":"2023-04-27T13:47:15.876176Z","shell.execute_reply":"2023-04-27T15:17:54.844582Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"\n======== Epoch 1 / 1 ========\nTraining...\n  Batch   100  of  13,126.    Elapsed: 0:00:41.\n  Batch   200  of  13,126.    Elapsed: 0:01:22.\n  Batch   300  of  13,126.    Elapsed: 0:02:04.\n  Batch   400  of  13,126.    Elapsed: 0:02:45.\n  Batch   500  of  13,126.    Elapsed: 0:03:27.\n  Batch   600  of  13,126.    Elapsed: 0:04:08.\n  Batch   700  of  13,126.    Elapsed: 0:04:49.\n  Batch   800  of  13,126.    Elapsed: 0:05:31.\n  Batch   900  of  13,126.    Elapsed: 0:06:12.\n  Batch 1,000  of  13,126.    Elapsed: 0:06:54.\n  Batch 1,100  of  13,126.    Elapsed: 0:07:35.\n  Batch 1,200  of  13,126.    Elapsed: 0:08:17.\n  Batch 1,300  of  13,126.    Elapsed: 0:08:58.\n  Batch 1,400  of  13,126.    Elapsed: 0:09:40.\n  Batch 1,500  of  13,126.    Elapsed: 0:10:21.\n  Batch 1,600  of  13,126.    Elapsed: 0:11:02.\n  Batch 1,700  of  13,126.    Elapsed: 0:11:44.\n  Batch 1,800  of  13,126.    Elapsed: 0:12:25.\n  Batch 1,900  of  13,126.    Elapsed: 0:13:07.\n  Batch 2,000  of  13,126.    Elapsed: 0:13:48.\n  Batch 2,100  of  13,126.    Elapsed: 0:14:30.\n  Batch 2,200  of  13,126.    Elapsed: 0:15:11.\n  Batch 2,300  of  13,126.    Elapsed: 0:15:53.\n  Batch 2,400  of  13,126.    Elapsed: 0:16:34.\n  Batch 2,500  of  13,126.    Elapsed: 0:17:15.\n  Batch 2,600  of  13,126.    Elapsed: 0:17:57.\n  Batch 2,700  of  13,126.    Elapsed: 0:18:38.\n  Batch 2,800  of  13,126.    Elapsed: 0:19:20.\n  Batch 2,900  of  13,126.    Elapsed: 0:20:01.\n  Batch 3,000  of  13,126.    Elapsed: 0:20:43.\n  Batch 3,100  of  13,126.    Elapsed: 0:21:24.\n  Batch 3,200  of  13,126.    Elapsed: 0:22:06.\n  Batch 3,300  of  13,126.    Elapsed: 0:22:47.\n  Batch 3,400  of  13,126.    Elapsed: 0:23:28.\n  Batch 3,500  of  13,126.    Elapsed: 0:24:10.\n  Batch 3,600  of  13,126.    Elapsed: 0:24:51.\n  Batch 3,700  of  13,126.    Elapsed: 0:25:33.\n  Batch 3,800  of  13,126.    Elapsed: 0:26:14.\n  Batch 3,900  of  13,126.    Elapsed: 0:26:56.\n  Batch 4,000  of  13,126.    Elapsed: 0:27:37.\n  Batch 4,100  of  13,126.    Elapsed: 0:28:19.\n  Batch 4,200  of  13,126.    Elapsed: 0:29:00.\n  Batch 4,300  of  13,126.    Elapsed: 0:29:42.\n  Batch 4,400  of  13,126.    Elapsed: 0:30:23.\n  Batch 4,500  of  13,126.    Elapsed: 0:31:04.\n  Batch 4,600  of  13,126.    Elapsed: 0:31:46.\n  Batch 4,700  of  13,126.    Elapsed: 0:32:27.\n  Batch 4,800  of  13,126.    Elapsed: 0:33:09.\n  Batch 4,900  of  13,126.    Elapsed: 0:33:50.\n  Batch 5,000  of  13,126.    Elapsed: 0:34:32.\n  Batch 5,100  of  13,126.    Elapsed: 0:35:13.\n  Batch 5,200  of  13,126.    Elapsed: 0:35:54.\n  Batch 5,300  of  13,126.    Elapsed: 0:36:36.\n  Batch 5,400  of  13,126.    Elapsed: 0:37:17.\n  Batch 5,500  of  13,126.    Elapsed: 0:37:59.\n  Batch 5,600  of  13,126.    Elapsed: 0:38:40.\n  Batch 5,700  of  13,126.    Elapsed: 0:39:22.\n  Batch 5,800  of  13,126.    Elapsed: 0:40:03.\n  Batch 5,900  of  13,126.    Elapsed: 0:40:44.\n  Batch 6,000  of  13,126.    Elapsed: 0:41:26.\n  Batch 6,100  of  13,126.    Elapsed: 0:42:07.\n  Batch 6,200  of  13,126.    Elapsed: 0:42:49.\n  Batch 6,300  of  13,126.    Elapsed: 0:43:30.\n  Batch 6,400  of  13,126.    Elapsed: 0:44:12.\n  Batch 6,500  of  13,126.    Elapsed: 0:44:53.\n  Batch 6,600  of  13,126.    Elapsed: 0:45:35.\n  Batch 6,700  of  13,126.    Elapsed: 0:46:16.\n  Batch 6,800  of  13,126.    Elapsed: 0:46:57.\n  Batch 6,900  of  13,126.    Elapsed: 0:47:39.\n  Batch 7,000  of  13,126.    Elapsed: 0:48:20.\n  Batch 7,100  of  13,126.    Elapsed: 0:49:02.\n  Batch 7,200  of  13,126.    Elapsed: 0:49:43.\n  Batch 7,300  of  13,126.    Elapsed: 0:50:25.\n  Batch 7,400  of  13,126.    Elapsed: 0:51:06.\n  Batch 7,500  of  13,126.    Elapsed: 0:51:47.\n  Batch 7,600  of  13,126.    Elapsed: 0:52:29.\n  Batch 7,700  of  13,126.    Elapsed: 0:53:10.\n  Batch 7,800  of  13,126.    Elapsed: 0:53:52.\n  Batch 7,900  of  13,126.    Elapsed: 0:54:33.\n  Batch 8,000  of  13,126.    Elapsed: 0:55:15.\n  Batch 8,100  of  13,126.    Elapsed: 0:55:56.\n  Batch 8,200  of  13,126.    Elapsed: 0:56:38.\n  Batch 8,300  of  13,126.    Elapsed: 0:57:19.\n  Batch 8,400  of  13,126.    Elapsed: 0:58:00.\n  Batch 8,500  of  13,126.    Elapsed: 0:58:42.\n  Batch 8,600  of  13,126.    Elapsed: 0:59:23.\n  Batch 8,700  of  13,126.    Elapsed: 1:00:05.\n  Batch 8,800  of  13,126.    Elapsed: 1:00:46.\n  Batch 8,900  of  13,126.    Elapsed: 1:01:28.\n  Batch 9,000  of  13,126.    Elapsed: 1:02:09.\n  Batch 9,100  of  13,126.    Elapsed: 1:02:51.\n  Batch 9,200  of  13,126.    Elapsed: 1:03:32.\n  Batch 9,300  of  13,126.    Elapsed: 1:04:14.\n  Batch 9,400  of  13,126.    Elapsed: 1:04:55.\n  Batch 9,500  of  13,126.    Elapsed: 1:05:36.\n  Batch 9,600  of  13,126.    Elapsed: 1:06:18.\n  Batch 9,700  of  13,126.    Elapsed: 1:06:59.\n  Batch 9,800  of  13,126.    Elapsed: 1:07:41.\n  Batch 9,900  of  13,126.    Elapsed: 1:08:22.\n  Batch 10,000  of  13,126.    Elapsed: 1:09:04.\n  Batch 10,100  of  13,126.    Elapsed: 1:09:45.\n  Batch 10,200  of  13,126.    Elapsed: 1:10:27.\n  Batch 10,300  of  13,126.    Elapsed: 1:11:08.\n  Batch 10,400  of  13,126.    Elapsed: 1:11:49.\n  Batch 10,500  of  13,126.    Elapsed: 1:12:31.\n  Batch 10,600  of  13,126.    Elapsed: 1:13:12.\n  Batch 10,700  of  13,126.    Elapsed: 1:13:54.\n  Batch 10,800  of  13,126.    Elapsed: 1:14:35.\n  Batch 10,900  of  13,126.    Elapsed: 1:15:17.\n  Batch 11,000  of  13,126.    Elapsed: 1:15:58.\n  Batch 11,100  of  13,126.    Elapsed: 1:16:40.\n  Batch 11,200  of  13,126.    Elapsed: 1:17:21.\n  Batch 11,300  of  13,126.    Elapsed: 1:18:02.\n  Batch 11,400  of  13,126.    Elapsed: 1:18:44.\n  Batch 11,500  of  13,126.    Elapsed: 1:19:25.\n  Batch 11,600  of  13,126.    Elapsed: 1:20:07.\n  Batch 11,700  of  13,126.    Elapsed: 1:20:48.\n  Batch 11,800  of  13,126.    Elapsed: 1:21:30.\n  Batch 11,900  of  13,126.    Elapsed: 1:22:11.\n  Batch 12,000  of  13,126.    Elapsed: 1:22:53.\n  Batch 12,100  of  13,126.    Elapsed: 1:23:34.\n  Batch 12,200  of  13,126.    Elapsed: 1:24:15.\n  Batch 12,300  of  13,126.    Elapsed: 1:24:57.\n  Batch 12,400  of  13,126.    Elapsed: 1:25:38.\n  Batch 12,500  of  13,126.    Elapsed: 1:26:20.\n  Batch 12,600  of  13,126.    Elapsed: 1:27:01.\n  Batch 12,700  of  13,126.    Elapsed: 1:27:43.\n  Batch 12,800  of  13,126.    Elapsed: 1:28:24.\n  Batch 12,900  of  13,126.    Elapsed: 1:29:06.\n  Batch 13,000  of  13,126.    Elapsed: 1:29:47.\n  Batch 13,100  of  13,126.    Elapsed: 1:30:28.\n\n  Average training loss: 0.68\n  Training epcoh took: 1:30:39\n\nTraining complete!\nTotal training took 1:30:39 (h:mm:ss)\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'deberta_weights.pth')","metadata":{"id":"In2BKayGHB60","execution":{"iopub.status.busy":"2023-04-27T15:17:54.848360Z","iopub.execute_input":"2023-04-27T15:17:54.849063Z","iopub.status.idle":"2023-04-27T15:17:55.643558Z","shell.execute_reply.started":"2023-04-27T15:17:54.849018Z","shell.execute_reply":"2023-04-27T15:17:55.642506Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"model.load_state_dict(torch.load('deberta_weights.pth'))","metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:17:55.644968Z","iopub.execute_input":"2023-04-27T15:17:55.645332Z","iopub.status.idle":"2023-04-27T15:17:55.926182Z","shell.execute_reply.started":"2023-04-27T15:17:55.645295Z","shell.execute_reply":"2023-04-27T15:17:55.925002Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"test_model(model)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:17:55.928624Z","iopub.execute_input":"2023-04-27T15:17:55.929285Z","iopub.status.idle":"2023-04-27T15:17:56.108231Z","shell.execute_reply.started":"2023-04-27T15:17:55.929246Z","shell.execute_reply":"2023-04-27T15:17:56.107042Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"                   -де            -д^е  target_idx      prediction  is_right\n331232   додумывавшими  дод^умывавшими           3  дод^умывавшими         1\n647800        маринова       мар^инова           3       мар^инова         1\n431832    зарядившимся   заряд^ившимся           5   заряд^ившимся         1\n424960      заполучаем     заполуч^аем           7     заполуч^аем         1\n1643854          шлихи          шлих^и           4          шл^ихи         0\n442768        затечешь       затеч^ешь           5       затеч^ешь         1\n563471     комплементе    комплем^енте           7    комплем^енте         1\n819140    обмельчавшей   обмельч^авшей           7   обмельч^авшей         1\n1653022       щурившим       щ^урившим           1       щур^ившим         0\n1363772    сдружаетесь    сдруж^аетесь           5    сдруж^аетесь         1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>-де</th>\n      <th>-д^е</th>\n      <th>target_idx</th>\n      <th>prediction</th>\n      <th>is_right</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>331232</th>\n      <td>додумывавшими</td>\n      <td>дод^умывавшими</td>\n      <td>3</td>\n      <td>дод^умывавшими</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>647800</th>\n      <td>маринова</td>\n      <td>мар^инова</td>\n      <td>3</td>\n      <td>мар^инова</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>431832</th>\n      <td>зарядившимся</td>\n      <td>заряд^ившимся</td>\n      <td>5</td>\n      <td>заряд^ившимся</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>424960</th>\n      <td>заполучаем</td>\n      <td>заполуч^аем</td>\n      <td>7</td>\n      <td>заполуч^аем</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1643854</th>\n      <td>шлихи</td>\n      <td>шлих^и</td>\n      <td>4</td>\n      <td>шл^ихи</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>442768</th>\n      <td>затечешь</td>\n      <td>затеч^ешь</td>\n      <td>5</td>\n      <td>затеч^ешь</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>563471</th>\n      <td>комплементе</td>\n      <td>комплем^енте</td>\n      <td>7</td>\n      <td>комплем^енте</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>819140</th>\n      <td>обмельчавшей</td>\n      <td>обмельч^авшей</td>\n      <td>7</td>\n      <td>обмельч^авшей</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1653022</th>\n      <td>щурившим</td>\n      <td>щ^урившим</td>\n      <td>1</td>\n      <td>щур^ившим</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1363772</th>\n      <td>сдружаетесь</td>\n      <td>сдруж^аетесь</td>\n      <td>5</td>\n      <td>сдруж^аетесь</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"id":"QcKEfNpJHB36"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Заключение","metadata":{}},{"cell_type":"markdown","source":"Последние 2 модели слишком долго обучались, поэтому не получилось прогнать их через валидацию (время как на трейне) =((( Но на инференсе видно, что они неплохо обучились, как и bert, accuracy которого составила 0.79","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}